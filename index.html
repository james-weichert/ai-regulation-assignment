<!DOCTYPE html>
<html>
    <head>
        <title>Nifty Assignment: AI Regulation</title>

        <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">

        <style>
            html {
                font-family: 'Open Sans', sans-serif;
            }

            main {
                width: 80vw;
                margin-left: 10vw;
                margin-top: 5vh;

                padding-bottom: 5em;
            }

            h5 {
                 font-weight: normal;
                 font-size: 1.1em;
            }

            table {
                border-collapse: collapse;
                border-top: 0.1em solid #CCC;
            }
            tr {
                border-bottom: 0.1em solid #CCC;
            }
            td {
                padding-left: 1em;
                padding-right: 1em;
            }
            
        </style>
    </head>
    <body>

        <main>

            <h1>AI Regulation Assignment</h1>
            <h5>SIGCSE 2026 Nifty Assignment Submission</h5>

            <br />

            <h3>Description</h3>

            <p>
                The <em>AI Regulation Assignment</em> is an experimental 'hands-on' assignment designed for advanced undergraduate computing courses on 
                artificial intelligence, machine learning, and/or the social impacts of computing (i.e. CS ethics courses). In this assignment, 
                students are challenged to jailbreak and then align an open source large language model (LLM) relative to specific desired behavior.
            </p>
            <p>
                In doing so, students should become aware of (some of) the limitations of current LLM safety mechanisms, and are encouraged to explore 
                the difficult challenge of aligning AI models through a combination of fine-tuning, hard-coded filters, and policies to govern the use of AI.
            </p>
            <p>
                The assignment has three learning objectives:
            </p>
            <ul>
                <li><strong>Recognize</strong> limitations in the alignment of large language models, and <strong>describe</strong> how and why these limitations 
                    can be used to produce undesired outputs.</li>
                <li><strong>Apply</strong> understanding of AI alignment limitations to jailbreak and then subsequently align an open source LLM.</li>
                <li><strong>Evaluate</strong> the success of technical alignment interventions, and <strong>consider</strong> the role of policy
                and governance structures in the responsible development and deployment of AI.</li>
            </ul>

            <p>
                The Python notebook provided in this submission is an extension of an earlier version of the <em>AI Regulation Assignment</em> with 
                similar instructions, which was piloted with approximately 100 undergraduate and graduate students in a machine learning capstone 
                course and a CS ethics course, respectively. The new form of the assignment we present here has two advantages: 1) we use an open source 
                model as the jailbreaking/alignment target, mitigating concerns relating to terms of service breaches for jailbreaks attempted on commercial LLMs; 
                and 2) using the open source LLM, students have full access to the model itself to experiment with alignment techniques. The <a href="jailbreak_examples.html"><code>jailbreak_examples.html</code></a>
                file contains examples of the types of the jailbreaking attacks explored by students in the original pilot of this assignment (i.e. without the Python notebook).
            </p>

            <p>We provide an example rubric for assessing student submissions in the <a href="./rubric.html"><code>rubric.html</code></a> file.</p>

            <h4>Accessing the Assignment</h4>
            
            <p>The assignment (including student-facing instructions) is self-contained in the <code>ai_regulation_assignment.ipynb</code> file in this directory. We recommend uploading the file to <a href="https://colab.research.google.com/" target="_blank">Google Colab</a> to take advantage of the free T4 GPU runtime (more instructions in the assignment).
                As a backup, we have provided a PDF printout of the Python notebook contents (<a href="./ai_regulation_assignment_hardcopy.pdf" target="_blank"><code>ai_regulation_assignment_hardcopy.pdf</code></a>).
            </p>

            <br />

            <h3>Metadata</h3>

            <table>
                <tr>
                    <td><h4>Summary</h4></td>
                    <td>
                        <p>
                            In this Python notebook assignment (intended for use on Google Colab or a similar hosted <code>.ipynb</code> environment), students are challenged to jailbreak and then align an open source large language model (LLM) relative to specific desired behavior. 
                            The assignment scaffolding provides an example of jailbreak attacks using prompt engineering and LLM alignment using a system prompt and a hard-coded word filter. 
                            Students are then tasked with exploring the robustness of a jailbreak attack and an  alignment approach of their choosing to different user prompts.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Topics</h4></td>
                    <td>
                        <p>
                            This is a largely conceptual and exploratory assignment, focusing on the topics of AI (specially, LLM) safety and alignment, prompt engineering, 
                            and the regulation of AI through policy and governance.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Audience</h4></td>
                    <td>
                        <p>
                            The assignment assumes no prior (technical) knowledge of large language models and requires only introductory-level Python programming ability. 
                            However, the subject of the assignment (i.e. LLM jailbreaking) has the potential to be misused by students, and should therefore be treated 
                            with the same warnings as, for example, a buffer overflow exploitation assignment in a computer security course. As such, this assignment may 
                            be best suited as an addition to an AI course with additional content on large language models (e.g. the transformer architecture, using <code>HuggingFace</code> models), 
                            or a CS ethics course which discusses the social impacts and ethical challenges of current AI systems like ChatGPT.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Difficulty</h4></td>
                    <td>
                        <p>
                            We assumed no prior student knowledge of LLMs or LLM alignment in creating this assignment, as a key advantage of this work is its ability to adapt to different course settings. 
                            However, jailbreaking exploits can prove fickle and are not always predictable/replicable. Additionally, the configuration of a HuggingFace account and API token and the use of 
                            a Google Colab GPU runtime may be unfamiliar to students, requiring additional debugging support.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Length</h4></td>
                    <td>
                        <p>
                            The length of this assignment is variable given that a key component is the student's exploration of different jailbreaking and alignment approaches. On the lower end, the content of 
                            the entire assignment could be covered/worked through in a single 1-hour lab period, during which the instructor guides students through using the existing code scaffolding while 
                            giving students the opportunity to work in groups to test different prompting exploits. If given as homework, most students would likely only need to spend between 1 and 2 hours 
                            on the assignment.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Strengths</h4></td>
                    <td>
                        <p>We view the following as key strengths of this assignment:</p>

                        <ul>
                            <li>As a Google Colab notebook, the assignment (including use of the LLM API) is entirely self-contained, and can take advantage of free GPU compute using the Google Colab T4 runtime.</li>
                            <li>The low technical curve of this assignment is intentional, allowing students to focus more on understanding the extent of alignment mechanisms and to consider the wider social and ethical implications of deploying LLMs with critical alignment weaknesses.</li> 
                            <li>The assignment is suitable for a variety of courses, including both technical AI/ML courses and courses on the social impacts of computing and AI.</li>
                            <li>The open source model we use throughout the assignment mitigates concerns relating to potential terms of service breaches, and allows for more elaborate alignment interventions involving supervised fine-tuning (SFT) or manual adjustments to the model architecture.</li> 
                            <li>The purpose of this assignment is to bridge abstract discussions of 'responsible AI' with concrete technical interventions, enabling students to identify the challenges in applying normative policy preferences (e.g. the LLM should not use inappropriate language) to the training and deployment of AI systems. We see the <em>AI Regulation Assignment</em> as a successful first step in this direction.</li>
                            <li>The assignment is an inherently creative one, and the learning experience of all students is likely augmented by the sharing of jailbreak and alignment ideas from students with diverse backgrounds and perspectives. Being confronted with new creative challenges (jailbreaking) and solutions (alignment), students should begin to understand the need for a more holistic and consultative approach to AI safety, which emphasizes the value of diverse perspectives in defining and implementing policies for AI regulation.</li>

                        </ul>
                    </td>
                </tr>
                <tr>
                    <td><h4>Weaknesses</h4></td>
                    <td>
                        <p>We also acknowledge a few important limitations of the assignment in its current form, which we offer as opportunities for future improvement by ourselves or others in the CS education community:</p>

                        <ul>
                            <li>AI alignment and safety research necessarily requires interaction with misaligned model responses that could be inappropriate, offensive, or unethical. Nevertheless, we view it as an important part of the 'AI curriculum' for students to be confronted with these deficiencies, as they are largely inspired by examples from commercial AI models. 
                                In view of the sensitive nature of some jailbreaks, we have included an important warning at the beginning of the assignment notebook, and encourage instructors to set appropriate expectations for student interactions with this assignment. </li>
                            <li>The responses from the Mistral 7B LLM, like many large language models, are stochastic, and the authors are not aware of a mechanism for seeding randomness to ensure deterministic outputs. However, the stochasticity of LLM responses can be treated as a feature instead of bug, encouraging students to identify jailbreak and alignment techniques which work across a majority of possible responses.</li>
                        
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td><h4>Dependencies</h4></td>
                    <td>
                        <p>Since the assignment notebook is self-contained and the key model infrastructure is provided in the skeleton code, this assignment has minimal technical dependencies. In order to download the open source model, each user must sign up for a free account on <a href="https://huggingface.co/" target="_blank">HuggingFace</a>. The process for accessing the model is explained at the beginning of the notebook.</p>
                            
                        <p>Students with at least introductory-level understanding of Python (e.g. variables, function calls) should be prepared to tackle this largely creative assignment. We discuss a more technically complex variant of this assignment below.</p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Variants</h4></td>
                    <td>
                        <p><strong>Variant 1 (less complex)</strong></p>
                        <p>The simplest (and original) variant of this assignment requires no Python notebook and involves students testing the limits of prompt engineering-based jailbreaks on commercially-available large language models like ChatGPT or Grok. While this variant still provides a useful learning opportunity for students, we recommend against it in most cases given concerns about potential student breaches of terms of service for these commercial models. 
                            A key motivator for the creation of the Python notebook was to use an open source model that mitigates many of these concerns. Nonetheless, one option for time-constrained courses involves the instructor showcasing (and the class discussing) a few relatively harmless jailbreaks on a publicly available model (or using the Mistral 7B open source model) during lecture.</p>
                        
                        <p><strong>Variant 2 (more complex)</strong></p>
                            <p>For courses with more technical content on large language models, the Python notebook we provide could be built-upon to provide additional, more complex challenges for students. The <code>transformers</code> Python library used in the notebook provides full access to a completely-configurable <code>Mistral 7B</code> model (documentation is available <a href="https://huggingface.co/docs/transformers/model_doc/mistral" target="_blank">here</a>). With this variant, students could be tasked with investigating the effect of different model hyperparameters on the alignment of the model's responses.</p>
                            
                    </td>
                </tr>
                <tr>
                    <td><h4>Teaching Notes</h4></td>
                    <td>
                        <p>In our view, the most effective introduction to this assignment is achieved by walking through the first parts of the Python notebook in class (lecture, section, or lab) to familiarize students with the aim of the assignment, guide students through getting access to the model, and provide an example of how to prompt the model through the provided <code>chat()</code> function. The remainder of the notebook can be assigned for students to complete outside of class as homework.</li>
                        </p>
                    </td>
                </tr>
            </table>

        </main>

    </body>
</html>