<!DOCTYPE html>
<html>
    <head>
        <title>Nifty Assignment: AI Regulation</title>

        <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">

        <style>
            html {
                font-family: 'Open Sans', sans-serif;
            }

            main {
                width: 80vw;
                margin-left: 10vw;
                margin-top: 5vh;
            }

            h5 {
                 font-weight: normal;
                 font-size: 1.1em;
            }

            table {
                border-collapse: collapse;
                border-top: 0.1em solid #CCC;
            }
            tr {
                border-bottom: 0.1em solid #CCC;
            }
        </style>
    </head>
    <body>

        <main>

            <h1>AI Regulation Assignment</h1>
            <h5>SIGCSE 2026 Nifty Assignment Submission</h5>

            <br />

            <h3>Description</h3>

            <p>
                The <em>AI Regulation Assignment</em> is an experimental 'hands-on' assignment designed for advanced undergraduate computing courses on 
                artificial intelligence, machine learning, and/or the social impacts of computing (i.e. CS ethics courses). In this assignment, 
                students are challenged to jailbreak and then align an open source large language model (LLM) relative to specific desired behavior.
            </p>
            <p>
                In doing so, students should become aware of (some of) the limitations of current LLM safety mechanisms, and are encouraged to explore 
                the difficult challenge of aligning AI models through a combination of fine-tuning, hard-coded filters, and policies to govern the use of AI.
            </p>
            <p>
                The assignment has three learning objectives:
            </p>
            <ul>
                <li><strong>Recognize</strong> limitations in the alignment of large language models, and <strong>describe</strong> how and why these limitations 
                    can be used to produce undesired outputs.</li>
                <li><strong>Apply</strong> understanding of AI alignment limitations to jailbreak and then subsequently align an open source LLM.</li>
                <li><strong>Evaluate</strong> the success of technical alignment interventions, and <strong>consider</strong> the role of policy
                and governance structures in the responsible development and deployment of AI.</li>
            </ul>

            <p>
                The Python notebook provided in this submission is an extension of an earlier version of the <em>AI Regulation Assignment</em> with 
                similar instructions, which was piloted with approximately 100 undergraduate and graduate students in a machine learning capstone 
                course and a CS ethics course, respectively. The new form of the assignment we present here has two advantages: 1) we use an open source 
                model as the jailbreaking/alignment target, mitigating concerns relating to terms of service breaches for jailbreaks attempted on commercial LLMs; 
                and 2) using the open source LLM, students have full access to the model itself to experiment with alignment techniques. The <a href="jailbreak_examples.html"><code>jailbreak_examples.html</code></a>
                file contains examples of the types of the jailbreaking attacks explored by students in the original pilot of this assignment (i.e. without the Python notebook).
            </p>

            <p>We provide an example rubric for assessing student submissions in the <a href="./rubric.html"><code>rubric.html</code></a> file.</p>

            <h3>Metadata</h3>

            <table>
                <tr>
                    <td><h4>Summary</h4></td>
                    <td>
                        <p>
                            In this Python notebook assignment, students are challenged to jailbreak and then align an open source large language model (LLM) relative to specific desired behavior. 
                            The assignment scaffolding provides an example of jailbreak attacks using prompt engineering and LLM alignment using a system prompt and a hard-coded word filter. 
                            Students are then tasked with exploring the robustness of a jailbreak attack and an  alignment approach of their choosing to different user prompts.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Topics</h4></td>
                    <td>
                        <p>
                            This is a largely conceptual and exploratory assignment, focusing on the topics of AI (specially, LLM) safety and alignment, prompt engineering, 
                            and the regulation of AI through policy and governance.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Audience</h4></td>
                    <td>
                        <p>
                            The assignment assumes no prior (technical) knowledge of large language models and requires only introductory-level Python programming ability. 
                            However, the subject of the assignment (i.e. LLM jailbreaking) has the potential to be misused by students, and should therefore be treated 
                            with the same warnings as, for example, a buffer overflow exploitation assignment in a computer security course. As such, this assignment may 
                            be best suited as an addition to an AI course with additional content on large language models (e.g. the transformer architecture, using <code>HuggingFace</code> models), 
                            or a CS ethics course which discusses the social impacts and ethical challenges of current AI systems like ChatGPT.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Difficulty</h4></td>
                    <td>
                        <p>
                            We assumed no prior student knowledge of LLMs or LLM alignment in creating this assignment, as a key advantage of this work is its ability to adapt to different course settings. 
                            However, jailbreaking exploits can prove fickle and are not always predictable/replicable. Additionally, the configuration of a HuggingFace account and API token, and the use of 
                            a Google Colab GPU runtime may be unfamiliar to students and require additional debugging support.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Length</h4></td>
                    <td>
                        <p>
                            The length of this assignment is variable given a key component is the student's exploration of different jailbreaking and alignment approaches. On the lower end, the content of 
                            the entire assignment could be covered/worked through in a single 1-hour lab period, during which the instructor guides students through using the existing code scaffolding while 
                            giving students the opportunity to work in groups to test different prompting exploits. If given as homework, most students would likely only need to spend between 1 and 2 hours 
                            on the assignment.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td><h4>Strengths</h4></td>
                    <td>
                        We view the following as key strengths of this assignment:

                        <ul>
                            <li>As a Google Colab notebook, the assignment (including use of the LLM API) is entirely self-contained, and can take advantage of free GPU compute using the Google Colab T4 runtime.</li>
                            <li>The low technical curve of this assignment is intentional, allowing students to focus more on understanding the extent of alignment mechanisms and to consider the wider social and ethical implications of deploying LLMs with critical alignment weaknesses. 
                                This makes the assignment suitable for a variety of courses, including both technical AI/ML courses and courses on the social impacts of computing and AI.</li>
                            <li>The open source model we use throughout the assignment provides two key advantages over a commercially available LLM. First, we are much less concerned about potential terms of service breaches when the assignment is used appropriately in an educational setting. 
                                Second, the open source nature of the model could allow interested students to attempt more elaborate alignment interventions involving supervised fine-tuning (SFT), manual adjustments to the model architecture or parameters, or other white-box model features. This option is not explicitly mentioned in the current assignment, but the choice of model provides excellent optionality for future variants.</li> 
                            <li>The purpose of this assignment is to bridge abstract discussions of 'responsible AI' with concrete technical interventions, enabling students to identify the challenges in applying normative policy preferences (e.g. the LLM should not use inappropriate language) to the training and deployment of AI systems. We see the <em>AI Regulation Assignment</em> as a successful first step in this direction.</li>
                            <li>The assignment is an inherently creative one, and the learning experience of all students is likely augmented by the sharing of jailbreak and alignment ideas from students with diverse backgrounds and perspectives. Being confronted with new creative challenges (jailbreaking) and solutions (alignment), students should begin to understand the need for a more holistic and consultative approach to AI safety, which emphasizes the value of diverse perspectives in defining and implementing policies for the AI regulation.</li>

                        </ul>
                    </td>
                </tr>
                <tr>
                    <td><h4>Weaknesses</h4></td>
                    <td>
                        We also acknowledge a few important limitations of the assignment in its current form, which we offer as opportunities for future improvement by ourselves or others in the CS education community:

                        <ul>
                            <li>The topic of AI alignment and safety necessarily requires researcher (in this case, student) interaction with misaligned model responses that could be inappropriate, offensive, unethical, or which encourage or enable illegal activity. Nevertheless, we view it as an important part of the 'AI curriculum' for students to be confronted with these deficiencies, as they are largely inspired by examples from commercial AI models. 
                                In view of the sensitive nature of some jailbreaks, we have included an important warning at the beginning of the assignment notebook, and encourage instructors to set appropriate expectations for student interactions with this assignment. Additionally, while we view the “get the model to swear” jailbreak as a relatively harmless yet realistic example, other instructors may decide to take another approach as a guiding example. We would encourage this adaptation, as we view the flexibility to different (course) contexts as a key strength of this project.</li>
                            <li>The responses from the Mistral 7B LLM, like many large language models, are stochastic, and the authors are not aware of a mechanism for seeding randomness to ensure deterministic outputs. This presents the challenge that the intended model outputs around which we have designed some exercises or examples in this assignment may not appear when the corresponding code cell is actually run by a student or instructor. However, the stochasticity of LLM responses can be treated as a feature instead of bug, encouraging students to identify jailbreak and alignment techniques which work across a majority of possible responses.</li>
                        
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td><h4>Dependencies</h4></td>
                    <td></td>
                </tr>
                <tr>
                    <td><h4>Variants</h4></td>
                    <td></td>
                </tr>
                <tr>
                    <td><h4>Teaching Notes</h4></td>
                </tr>
            </table>

        </main>

    </body>
</html>